{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82ee39f-cb17-4ac5-9b12-dedaebbf50b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline_mean] MAE = 30.48\n",
      "ðŸƒ View run baseline_mean at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0/runs/2b77e4baa0614cf0b4f6c172ab3cc704\n",
      "ðŸ§ª View experiment at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 43.205148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 22:36:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lgbm_28lag] MAE = 8.38 | Î” = 22.10 (72.5%)\n",
      "ðŸƒ View run lgbm_28lag at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0/runs/b7a331ee96d648eb872ee82fd7249185\n",
      "ðŸ§ª View experiment at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 43.205148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43.205148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 22:37:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lgbm_top10_imp] MAE = 8.35 | Î” = 22.13 (72.6%)\n",
      "ðŸƒ View run lgbm_top10_imp at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0/runs/19ca43ba03084a448c06c10e451f35d6\n",
      "ðŸ§ª View experiment at: https://dagshub.com/manogna145/cda_finalproject.mlflow/#/experiments/0\n",
      "\n",
      "âœ… Best model: 'topk' with MAE = 8.35\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_and_log_all.py\n",
    "\n",
    "Loads cleaned Citi Bike data (hourly counts), then:\n",
    "  1) logs a baseline mean model\n",
    "  2) logs a LightGBM on 28 lag features\n",
    "  3) logs a LightGBM on top-10 importance features\n",
    "\n",
    "Each run logs:\n",
    "  - mae\n",
    "  - mae_improvement = baseline_mae - mae\n",
    "  - pct_improvement = (baseline_mae - mae) / baseline_mae\n",
    "  - relevant parameters (train_frac, max_lag, top_k)\n",
    "\n",
    "At the end, prints out which model performed best (lowest MAE).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DagsHub MLflow settings\n",
    "env_user = \"manogna145\"\n",
    "env_pwd  = \"2207a6e5841298a92645f15b687a350ec0044c23\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = env_user\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = env_pwd\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/manogna145/cda_finalproject.mlflow\")\n",
    "mlflow.set_experiment(\"CitiBike_TripDuration_Models\")\n",
    "\n",
    "# Data & split config\n",
    "PARQUET_PATH = \"/Users/manu/Desktop/cda_final/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "TRAIN_FRAC   = 0.8\n",
    "MAX_LAG      = 28\n",
    "TOP_K        = 10\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def load_and_agg(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the cleaned parquet, aggregates ride counts per hour, and returns a time series DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['datetime']   = df['started_at'].dt.floor('H')\n",
    "    agg = df.groupby('datetime').size().reset_index(name='count')\n",
    "    return agg.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def train_test_split_ts(df: pd.DataFrame, frac: float):\n",
    "    \"\"\"\n",
    "    Splits time series DataFrame into train and test based on fraction.\n",
    "    \"\"\"\n",
    "    split_idx = int(len(df) * frac)\n",
    "    return df.iloc[:split_idx].reset_index(drop=True), df.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def log_baseline(train: pd.DataFrame, test: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Logs a baseline model that predicts the mean of the training counts.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=\"baseline_mean\"):\n",
    "        mlflow.log_param(\"model_type\", \"baseline_mean\")\n",
    "        mlflow.log_param(\"train_frac\", TRAIN_FRAC)\n",
    "        mean_pred = train['count'].mean()\n",
    "        preds = [mean_pred] * len(test)\n",
    "        mae = mean_absolute_error(test['count'], preds)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mae_improvement\", 0.0)\n",
    "        mlflow.log_metric(\"pct_improvement\", 0.0)\n",
    "        print(f\"[baseline_mean] MAE = {mae:.2f}\")\n",
    "    return mae\n",
    "\n",
    "\n",
    "def log_lag_model(df: pd.DataFrame, baseline_mae: float) -> float:\n",
    "    \"\"\"\n",
    "    Creates 1â€“MAX_LAG lag features, trains a LightGBM, logs metrics and model.\n",
    "    \"\"\"\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag['count'].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feat_cols = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "    X_train, y_train = train[feat_cols], train['count']\n",
    "    X_test,  y_test  = test[feat_cols],  test['count']\n",
    "\n",
    "    with mlflow.start_run(run_name=\"lgbm_28lag\"):\n",
    "        mlflow.log_param(\"model_type\", \"lgbm_28lag\")\n",
    "        mlflow.log_param(\"train_frac\", TRAIN_FRAC)\n",
    "        mlflow.log_param(\"max_lag\", MAX_LAG)\n",
    "\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        imp_abs = baseline_mae - mae\n",
    "        imp_pct = imp_abs / baseline_mae if baseline_mae else 0.0\n",
    "\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mae_improvement\", imp_abs)\n",
    "        mlflow.log_metric(\"pct_improvement\", imp_pct)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"[lgbm_28lag] MAE = {mae:.2f} | Î” = {imp_abs:.2f} ({imp_pct:.1%})\")\n",
    "    return mae\n",
    "\n",
    "\n",
    "def log_topk_model(df: pd.DataFrame, baseline_mae: float) -> float:\n",
    "    \"\"\"\n",
    "    Selects top-K lag features by importance and trains a LightGBM on them.\n",
    "    \"\"\"\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag['count'].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feat_cols = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "\n",
    "    base = LGBMRegressor(random_state=42)\n",
    "    base.fit(train[feat_cols], train['count'])\n",
    "    importances = pd.Series(base.feature_importances_, index=feat_cols)\n",
    "    top_feats = importances.nlargest(TOP_K).index.tolist()\n",
    "\n",
    "    with mlflow.start_run(run_name=\"lgbm_top10_imp\"):\n",
    "        mlflow.log_param(\"model_type\", \"lgbm_top10_imp\")\n",
    "        mlflow.log_param(\"train_frac\", TRAIN_FRAC)\n",
    "        mlflow.log_param(\"max_lag\", MAX_LAG)\n",
    "        mlflow.log_param(\"top_k\", TOP_K)\n",
    "        mlflow.log_param(\"selected_feats\", top_feats)\n",
    "\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(train[top_feats], train['count'])\n",
    "        preds = model.predict(test[top_feats])\n",
    "        mae = mean_absolute_error(test['count'], preds)\n",
    "        imp_abs = baseline_mae - mae\n",
    "        imp_pct = imp_abs / baseline_mae if baseline_mae else 0.0\n",
    "\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mae_improvement\", imp_abs)\n",
    "        mlflow.log_metric(\"pct_improvement\", imp_pct)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"[lgbm_top10_imp] MAE = {mae:.2f} | Î” = {imp_abs:.2f} ({imp_pct:.1%})\")\n",
    "    return mae\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = load_and_agg(PARQUET_PATH)\n",
    "    train, test = train_test_split_ts(df, TRAIN_FRAC)\n",
    "\n",
    "    baseline = log_baseline(train, test)\n",
    "    lag_mae  = log_lag_model(df, baseline)\n",
    "    topk_mae = log_topk_model(df, baseline)\n",
    "\n",
    "    maes = {\"baseline\": baseline, \"lag\": lag_mae, \"topk\": topk_mae}\n",
    "    best = min(maes, key=maes.get)\n",
    "    print(f\"\\nâœ… Best model: '{best}' with MAE = {maes[best]:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf04a7c-9c3c-499d-a35c-001c663a141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models into: /Users/manu/Desktop/cda_final/notebook\n",
      "[baseline_mean] MAE = 30.4785, saved to /Users/manu/Desktop/cda_final/notebook/baseline_mean.pkl\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 43.205148\n",
      "[lgbm_28lag] MAE = 8.3772, saved to /Users/manu/Desktop/cda_final/notebook/lgbm_28lag.pkl\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 43.205148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 6527, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43.205148\n",
      "[lgbm_top10_imp] MAE = 8.3522, saved to /Users/manu/Desktop/cda_final/notebook/lgbm_top10_imp.pkl\n",
      "âœ… Best model 'lgbm_top10_imp' (MAE=8.3522) saved to /Users/manu/Desktop/cda_final/notebook/best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_and_save_models.py\n",
    "\n",
    "Loads cleaned CitiBike data, then:\n",
    "  1) trains a baseline mean model\n",
    "  2) trains a LightGBM on 28 lag features\n",
    "  3) trains a LightGBM on top-10 importance features\n",
    "\n",
    "Saves each trained model locally under a specified notebook folder, and also saves the best-performing model as `best_model.pkl`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Data & split config\n",
    "PARQUET_PATH = \"/Users/manu/Desktop/cda_final/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "TRAIN_FRAC   = 0.8\n",
    "MAX_LAG      = 28\n",
    "TOP_K        = 10\n",
    "\n",
    "# Output directory for pickled models\n",
    "NOTEBOOK_DIR = Path(\"/Users/manu/Desktop/cda_final/notebook\")\n",
    "NOTEBOOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving models into:\", NOTEBOOK_DIR)\n",
    "\n",
    "# Load and aggregate hourly counts\n",
    "def load_and_agg(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df[\"datetime\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(\"datetime\")\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"datetime\")\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "# Split into train/test\n",
    "def train_test_split_ts(df, frac):\n",
    "    idx = int(len(df) * frac)\n",
    "    return df.iloc[:idx], df.iloc[idx:]\n",
    "\n",
    "# Train & save baseline model\n",
    "def train_baseline(train, test):\n",
    "    mean_val = train[\"count\"].mean()\n",
    "    preds = [mean_val] * len(test)\n",
    "    mae = mean_absolute_error(test[\"count\"], preds)\n",
    "    dummy = DummyRegressor(strategy=\"constant\", constant=mean_val)\n",
    "    dummy.fit(train[[\"count\"]], train[\"count\"])\n",
    "    out_path = NOTEBOOK_DIR / \"baseline_mean.pkl\"\n",
    "    joblib.dump(dummy, out_path)\n",
    "    print(f\"[baseline_mean] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "    return dummy, mae\n",
    "\n",
    "# Train & save 28-lag model\n",
    "def train_lag_model(df):\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag[\"count\"].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feats = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "    X_train, y_train = train[feats], train[\"count\"]\n",
    "    X_test,  y_test  = test[feats],  test[\"count\"]\n",
    "\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    out_path = NOTEBOOK_DIR / \"lgbm_28lag.pkl\"\n",
    "    joblib.dump(model, out_path)\n",
    "    print(f\"[lgbm_28lag] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "    return model, mae\n",
    "\n",
    "# Train & save top-K importance model\n",
    "def train_topk_model(df):\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag[\"count\"].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feats = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "\n",
    "    base = LGBMRegressor(random_state=42)\n",
    "    base.fit(train[feats], train[\"count\"])\n",
    "    importances = pd.Series(base.feature_importances_, index=feats)\n",
    "    top_feats = importances.nlargest(TOP_K).index.tolist()\n",
    "\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(train[top_feats], train[\"count\"])\n",
    "    preds = model.predict(test[top_feats])\n",
    "    mae = mean_absolute_error(test[\"count\"], preds)\n",
    "    out_path = NOTEBOOK_DIR / \"lgbm_top10_imp.pkl\"\n",
    "    joblib.dump(model, out_path)\n",
    "    print(f\"[lgbm_top10_imp] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "    return model, mae\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    df = load_and_agg(PARQUET_PATH)\n",
    "    train, test = train_test_split_ts(df, TRAIN_FRAC)\n",
    "\n",
    "    baseline_model, mae_baseline = train_baseline(train, test)\n",
    "    lag_model,      mae_lag      = train_lag_model(df)\n",
    "    topk_model,     mae_topk     = train_topk_model(df)\n",
    "\n",
    "    metrics = {\n",
    "        \"baseline_mean\": mae_baseline,\n",
    "        \"lgbm_28lag\":    mae_lag,\n",
    "        \"lgbm_top10_imp\": mae_topk,\n",
    "    }\n",
    "    models = {\n",
    "        \"baseline_mean\": baseline_model,\n",
    "        \"lgbm_28lag\":    lag_model,\n",
    "        \"lgbm_top10_imp\": topk_model,\n",
    "    }\n",
    "    best_name = min(metrics, key=metrics.get)\n",
    "    best_model = models[best_name]\n",
    "    best_path  = NOTEBOOK_DIR / \"best_model.pkl\"\n",
    "    joblib.dump(best_model, best_path)\n",
    "    print(f\"âœ… Best model '{best_name}' (MAE={metrics[best_name]:.4f}) saved to {best_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872f6dd-bb54-4788-a0c6-f2e3db445351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
