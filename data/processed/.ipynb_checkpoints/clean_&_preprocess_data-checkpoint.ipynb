{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfbbd52-1825-4d54-812f-66e1cabf4b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,000,000 rows from 202312-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202301-citibike-tripdata_1.csv\n",
      "Loaded 204,874 rows from 202312-citibike-tripdata_3.csv\n",
      "Loaded 453,152 rows from 202305-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202312-citibike-tripdata_1.csv\n",
      "Loaded 795,412 rows from 202301-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202305-citibike-tripdata_1.csv\n",
      "Loaded 1,000,000 rows from 202305-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202305-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202306-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202309-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202309-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202306-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202306-citibike-tripdata_1.csv\n",
      "Loaded 1,000,000 rows from 202309-citibike-tripdata_1.csv\n",
      "Loaded 451,549 rows from 202306-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202311-citibike-tripdata_1.csv\n",
      "Loaded 696,171 rows from 202302-citibike-tripdata_2.csv\n",
      "Loaded 471,150 rows from 202309-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202311-citibike-tripdata_2.csv\n",
      "Loaded 816,977 rows from 202311-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202302-citibike-tripdata_1.csv\n",
      "Loaded 749,716 rows from 202304-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202304-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202304-citibike-tripdata_1.csv\n",
      "Loaded 1,000,000 rows from 202303-citibike-tripdata_1.csv\n",
      "Loaded 1,000,000 rows from 202310-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202310-citibike-tripdata_2.csv\n",
      "Loaded 964,180 rows from 202308-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202303-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202310-citibike-tripdata_1.csv\n",
      "Loaded 118,932 rows from 202303-citibike-tripdata_3.csv\n",
      "Loaded 659,581 rows from 202307-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202308-citibike-tripdata_1.csv\n",
      "Loaded 725,336 rows from 202310-citibike-tripdata_4.csv\n",
      "Loaded 1,000,000 rows from 202307-citibike-tripdata_1.csv\n",
      "Loaded 1,000,000 rows from 202308-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202307-citibike-tripdata_3.csv\n",
      "Loaded 1,000,000 rows from 202307-citibike-tripdata_2.csv\n",
      "Loaded 1,000,000 rows from 202308-citibike-tripdata_3.csv\n",
      "Total rows after concat: 35,107,030\n",
      "Top 3 start stations:\n",
      "  W 21 St & 6 Ave\n",
      "  Broadway & W 58 St\n",
      "  West St & Chambers St\n",
      "Dropped 794 invalid/duplicate rows\n",
      "\n",
      "✓ Saved cleaned data (365,696 rows) to\n",
      "  /Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "clean_preprocess.py\n",
    "\n",
    "Recursively loads all raw Citibike CSVs from CSV_DIR, parses & cleans them,\n",
    "filters to the top-K busiest start stations, engineers time-based features,\n",
    "and writes out a single Parquet for downstream modeling.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION — update these to your local paths\n",
    "CSV_DIR     = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/raw_citibike_csvs\"\n",
    "OUTPUT_DIR  = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"citibike_2023_top3.parquet\")\n",
    "TOP_K       = 3\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# exact columns present in your raw CSVs\n",
    "COLUMNS = [\n",
    "    \"ride_id\",\n",
    "    \"rideable_type\",\n",
    "    \"started_at\",\n",
    "    \"ended_at\",\n",
    "    \"start_station_name\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_name\",\n",
    "    \"end_station_id\",\n",
    "    \"start_lat\",\n",
    "    \"start_lng\",\n",
    "    \"end_lat\",\n",
    "    \"end_lng\",\n",
    "    \"member_casual\",\n",
    "]\n",
    "\n",
    "def load_all_csvs(csv_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Read & concatenate only the relevant columns from every CSV in csv_dir.\"\"\"\n",
    "    paths = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            usecols=COLUMNS,\n",
    "            dtype={\n",
    "                \"ride_id\": str,\n",
    "                \"rideable_type\": \"category\",\n",
    "                \"start_station_id\": str,\n",
    "                \"end_station_id\": str,\n",
    "                \"member_casual\": \"category\",\n",
    "            },\n",
    "        )\n",
    "        print(f\"Loaded {len(df):,} rows from {os.path.basename(p)}\")\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Total rows after concat: {len(combined):,}\")\n",
    "    return combined\n",
    "\n",
    "def parse_and_engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Parse timestamps, compute trip_duration_min & calendar features.\"\"\"\n",
    "    # cast to datetime\n",
    "    df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "    df[\"ended_at\"]   = pd.to_datetime(df[\"ended_at\"])\n",
    "    # duration in minutes (clip negatives to zero)\n",
    "    df[\"trip_duration_min\"] = (\n",
    "        (df[\"ended_at\"] - df[\"started_at\"])\n",
    "        .dt.total_seconds()\n",
    "        .div(60)\n",
    "        .clip(lower=0)\n",
    "    )\n",
    "    # extract hour and day-of-week\n",
    "    df[\"start_hour\"]      = df[\"started_at\"].dt.hour\n",
    "    df[\"start_dayofweek\"] = df[\"started_at\"].dt.dayofweek  # Monday=0\n",
    "    return df\n",
    "\n",
    "def filter_top_stations(df: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    \"\"\"Keep only trips from the k most-frequent start_station_name values.\"\"\"\n",
    "    top = (\n",
    "        df[\"start_station_name\"]\n",
    "          .value_counts()\n",
    "          .nlargest(k)\n",
    "          .index\n",
    "          .tolist()\n",
    "    )\n",
    "    print(f\"Top {k} start stations:\\n  \" + \"\\n  \".join(top))\n",
    "    return df[df[\"start_station_name\"].isin(top)].copy()\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicate ride_ids, nulls in key fields, and zero-duration trips.\"\"\"\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"ride_id\"])\n",
    "    df = df.dropna(subset=[\n",
    "        \"started_at\", \"ended_at\",\n",
    "        \"start_station_name\", \"end_station_name\",\n",
    "        \"trip_duration_min\"\n",
    "    ])\n",
    "    df = df[df[\"trip_duration_min\"] > 0]\n",
    "    print(f\"Dropped {before - len(df):,} invalid/duplicate rows\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # make sure output directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1. load & concat\n",
    "    df = load_all_csvs(CSV_DIR)\n",
    "\n",
    "    # 2. parse timestamps & engineer features\n",
    "    df = parse_and_engineer(df)\n",
    "\n",
    "    # 3. filter to top-K busy stations\n",
    "    df = filter_top_stations(df, TOP_K)\n",
    "\n",
    "    # 4. final clean\n",
    "    df = clean_df(df)\n",
    "\n",
    "    # 5. persist to Parquet\n",
    "    df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\n✓ Saved cleaned data ({len(df):,} rows) to\\n  {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e625a4-b0b0-4c6c-8b37-f07f0b52d3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
